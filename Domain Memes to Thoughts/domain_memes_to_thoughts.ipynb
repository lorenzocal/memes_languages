{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "gYpSxYTaZ6cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "import requests\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "Ig0UFQjOZ4cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define variables to deal with Gemini's request time limit"
      ],
      "metadata": {
        "id": "ccWTjSR-Zqui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "global TIME_LAST_REQUEST\n",
        "global TIME_BETWEEN_REQUESTS\n",
        "\n",
        "TIME_LAST_REQUEST = datetime.now()\n",
        "TIME_BETWEEN_REQUESTS = 4.5 #seconds\n",
        "# Gemini 1.5 Flash -> 15 RPM (1 req. each 4 s)"
      ],
      "metadata": {
        "id": "Zn0KuRB7Zn9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini queries"
      ],
      "metadata": {
        "id": "nh1P3n3sY7Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_llm():\n",
        "\n",
        "  global large_language_model\n",
        "\n",
        "  genai.configure(api_key=\"AIzaSyB1vpKTeuUfYr5Ad3OqUxvJy6vAPA-4SCc\")\n",
        "  large_language_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "load_llm()"
      ],
      "metadata": {
        "id": "6_O9Bs5_aWNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def general_prompt(descriptions):\n",
        "    # Join the descriptions into a single string\n",
        "    descriptions_text = \"\\n\".join([f\"- {desc}\" for desc in descriptions])\n",
        "\n",
        "    # Define the prompt template\n",
        "    prompt_template = (\n",
        "        \"A 'Meme' is a representation of a shared thought or idea, expressed through a combination of image and text, \"\n",
        "        \"that encapsulates a relatable, humorous, or culturally significant situation. It serves as a 'Template' for creating variations, \"\n",
        "        \"called 'Domain Memes', that convey specific instances of the broader concept, enabling collective understanding, communication, and humor.\\n\\n\"\n",
        "        \"I have the descriptions of several Domain Memes of the same Meme:\\n\\n\"\n",
        "        \"{descriptions}\\n\\n\"\n",
        "        \"Based on the descriptions, examples, and keywords, identify the unifying concept ('Meme') that connects these 'Domain Memes'. \"\n",
        "        \"The 'Meme' should capture the overarching theme or idea that applies to all the 'Domain Memes', highlighting their shared emotional tone, \"\n",
        "        \"social context, or cultural relevance. Focus on the meaning, ignore the name of the 'Meme' and its description. \"\n",
        "    )\n",
        "\n",
        "    # Insert the descriptions into the template\n",
        "    return prompt_template.format(descriptions=descriptions_text)"
      ],
      "metadata": {
        "id": "8O_pzmI9YyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def domain_memes_to_meme(prompt):\n",
        "\n",
        "    global TIME_LAST_REQUEST, TIME_BETWEEN_REQUESTS\n",
        "\n",
        "    current_time = datetime.now()\n",
        "    time_from_last_request = current_time - TIME_LAST_REQUEST\n",
        "\n",
        "    if time_from_last_request.seconds < TIME_BETWEEN_REQUESTS:\n",
        "        time_to_wait = TIME_BETWEEN_REQUESTS - time_from_last_request.seconds\n",
        "        time.sleep(time_to_wait)\n",
        "\n",
        "    TIME_LAST_REQUEST = datetime.now()\n",
        "\n",
        "    response = large_language_model.generate_content(\n",
        "    prompt,\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        temperature=0.5,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "igFZByX0Y208"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_thought(meme):\n",
        "\n",
        "  global TIME_LAST_REQUEST, TIME_BETWEEN_REQUESTS\n",
        "\n",
        "  current_time = datetime.now()\n",
        "  time_from_last_request = current_time - TIME_LAST_REQUEST\n",
        "\n",
        "  if time_from_last_request.seconds < TIME_BETWEEN_REQUESTS:\n",
        "    time_to_wait = TIME_BETWEEN_REQUESTS - time_from_last_request.seconds\n",
        "    time.sleep(time_to_wait)\n",
        "\n",
        "  TIME_LAST_REQUEST = datetime.now()\n",
        "\n",
        "  summarization = large_language_model.generate_content(\n",
        "    f\"Get the thought behind this meme in a triplet of 3 emotions: {meme}. Ignore concepts which are by nature associated with any meme (for example Irony, Humor, Relatability). The structure of the answer must be: Word1, Word2, Word3\",\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        #candidate_count=1,\n",
        "        #stop_sequences=[\"x\"],\n",
        "        #max_output_tokens=100,\n",
        "        temperature=1.5,\n",
        "        )\n",
        "    )\n",
        "  return summarization.text"
      ],
      "metadata": {
        "id": "tx4Jc7qRY5ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group by Meme and Language, storing Domain Memes in a list"
      ],
      "metadata": {
        "id": "NWo19tgFadLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_path = \"results.json\"\n",
        "\n",
        "with open(results_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "grouped_data = {}\n",
        "for item in data:\n",
        "    meme_name = item.get(\"Meme name\")\n",
        "    language = item.get(\"Language\")\n",
        "    explanation = item.get(\"Explaination\")\n",
        "\n",
        "    # Create nested dictionary structure for grouping\n",
        "    if meme_name not in grouped_data:\n",
        "        grouped_data[meme_name] = {}\n",
        "    if language not in grouped_data[meme_name]:\n",
        "        grouped_data[meme_name][language] = []\n",
        "\n",
        "    # Add explanation to the list\n",
        "    grouped_data[meme_name][language].append(explanation)\n",
        "\n",
        "grouped_results_path = \"grouped_results.json\"\n",
        "with open(grouped_results_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(grouped_data, json_file, ensure_ascii=False, indent=4)\n",
        "print(f\"Results saved to {grouped_results_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PRiUv-daejX",
        "outputId": "b76da564-208d-43da-b117-de1b13ec503f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to grouped_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the Thought for each Meme and Language"
      ],
      "metadata": {
        "id": "dzVeNBanYXQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instances_langauges_grouped = []\n",
        "columns = ['Meme name', 'Language', 'Meme', 'Thought']\n",
        "\n",
        "for meme in grouped_data:\n",
        "  for language in grouped_data[meme]:\n",
        "    explainations = grouped_data[meme][language]\n",
        "    prompt = general_prompt(explainations)\n",
        "    dm_to_meme = domain_memes_to_meme(prompt)\n",
        "    thought = get_thought(dm_to_meme)\n",
        "    instances_langauges_grouped.append([meme, language, dm_to_meme, thought])\n",
        "\n",
        "df = pd.DataFrame(instances_langauges_grouped, columns=columns)"
      ],
      "metadata": {
        "id": "pSzvoGeAYWo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('memes_thoughts.csv', index=False)"
      ],
      "metadata": {
        "id": "P8Dc6LAAdat2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}